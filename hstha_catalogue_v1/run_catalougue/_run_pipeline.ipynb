{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# from astropy.io import fits\n",
    "# from tqdm import tqdm \n",
    "# import numpy as np\n",
    "# import os\n",
    "# import gc \n",
    "# from glob import glob \n",
    "# import sys\n",
    "\n",
    "# sys.path.append('../')\n",
    "# from modules import cat_cutouts, cat_misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PyHSTHACat:\n",
    "#     def __init__(self, galaxy, galaxy_hst=None):\n",
    "#         \"\"\"\n",
    "#         Initialize the PyHSTHACat class with default parameters and file paths.\n",
    "#         \"\"\"\n",
    "#         # Define galaxy names\n",
    "#         self.galaxy = galaxy\n",
    "#         if galaxy_hst is None:\n",
    "#             self.galaxy_hst = self.galaxy\n",
    "\n",
    "#         # Define root directory\n",
    "#         self.rootdir = '/Users/abarnes/Dropbox/work/Smallprojects/galaxies'\n",
    "        \n",
    "#         # Define filenames\n",
    "#         self.hstha_file = f\"{self.rootdir}/data_hstha/{self.galaxy_hst}/hst_contsub/{self.galaxy_hst}_hst_ha.fits\"\n",
    "#         self.muscat_file = f\"{self.rootdir}/data_hstha/{self.galaxy_hst}/muse/{self.galaxy.upper()}_nebmask.fits\"\n",
    "#         musha_glob = f\"{self.rootdir}/data_hstha/{self.galaxy_hst}/muse/{self.galaxy.upper()}-*_MAPS.fits\"\n",
    "#         self.musha_file = glob(musha_glob)[0]  # Use glob to find the correct file\n",
    "\n",
    "#         # Define directories for outputs\n",
    "#         self.cutout_dir = f\"{self.rootdir}/data_hstha_nebulae_catalogue/{self.galaxy_hst}/cutouts\"\n",
    "#         self.catalogue_dir = f\"{self.rootdir}/data_hstha_nebulae_catalogue/{self.galaxy_hst}/catalogue\"\n",
    "#         self.cutouts_hdus_dir = f\"{self.rootdir}/data_hstha_nebulae_catalogue/{self.galaxy_hst}/cutouts_hdus\"\n",
    "\n",
    "#         # Flags for rerunning certain steps\n",
    "#         self.rerun_all = True\n",
    "#         self.rerun_regions = True\n",
    "#         self.rerun_masking = True\n",
    "\n",
    "#         # Define additional files\n",
    "#         self.regions_file = f\"{self.cutout_dir}/sample.reg\"\n",
    "#         self.regions_pickle_file = f\"{self.cutout_dir}/sample.pickel\"\n",
    "#         self.sample_table_file = f\"{self.rootdir}/data_misc/sample_table/phangs_sample_table_v1p6.fits\"\n",
    "#         self.muscat_table_file = f\"{self.rootdir}/data_misc/Nebulae_catalogue_v3/Nebulae_catalogue_v3.fits\"\n",
    "\n",
    "#     def make_paths(self):\n",
    "#         \"\"\"\n",
    "#         Create necessary directories for outputs. Deletes and recreates root directory if `rerun_all` is True.\n",
    "#         \"\"\"\n",
    "#         root_dir = f\"{self.rootdir}/data_hstha_nebulae_catalogue/{self.galaxy_hst}/\"\n",
    "#         print('[Info] Outputting to the following:')\n",
    "#         print(root_dir)\n",
    "\n",
    "#         # Remove and recreate root directory if rerun_all is set\n",
    "#         if self.rerun_all:\n",
    "#             os.system(f'rm -rf {root_dir}')\n",
    "\n",
    "#         # Create root directory if it doesn't exist\n",
    "#         if not os.path.isdir(root_dir):\n",
    "#             os.mkdir(root_dir)\n",
    "\n",
    "#         # Create subdirectories for cutouts, catalogues, and HDUs\n",
    "#         for path in [self.cutout_dir, self.catalogue_dir, self.cutouts_hdus_dir]:\n",
    "#             if not os.path.isdir(path):\n",
    "#                 os.mkdir(path)\n",
    "\n",
    "#     def load_files(self):\n",
    "#         \"\"\"\n",
    "#         Load FITS files and preprocess their data by handling NaNs and converting to float32 format.\n",
    "#         \"\"\"\n",
    "#         # Load FITS files\n",
    "#         self.hstha_hdu = fits.open(self.hstha_file)[0]\n",
    "#         self.musha_hdu = fits.open(self.musha_file)['HA6562_FLUX']\n",
    "#         self.muscat_hdu = fits.open(self.muscat_file)[0]\n",
    "\n",
    "#         # Update arrays to handle NaNs and ensure correct data type\n",
    "#         self.musha_hdu.data[np.isnan(self.musha_hdu.data)] = -100\n",
    "#         self.muscat_hdu.data = np.array(self.muscat_hdu.data, dtype=float)\n",
    "#         self.muscat_hdu.data[self.muscat_hdu.data == -1] = np.nan\n",
    "\n",
    "#         # Convert all HDUs to float32 format for compatibility\n",
    "#         hdus = [self.hstha_hdu, self.musha_hdu, self.muscat_hdu]\n",
    "#         self.hdus = [cat_misc.convert_to_float32(hdu.copy()) for hdu in hdus]\n",
    "\n",
    "#     def get_regions_and_sample_table(self):\n",
    "#         \"\"\"\n",
    "#         Retrieve regions and sample table. Uses existing region files if available unless rerun is flagged.\n",
    "#         \"\"\"\n",
    "#         if os.path.exists(self.regions_file) and not self.rerun_regions:\n",
    "#             self.regions = cat_misc.load_pickle(self.regions_pickle_file)\n",
    "#         else:\n",
    "#             # Generate regions from the sample table\n",
    "#             muscat_table = cat_misc.get_museprops(self.galaxy, self.muscat_table_file)\n",
    "#             regions_sky = cat_cutouts.get_ds9regions_all(muscat_table, outputfile=self.regions_file)\n",
    "#             self.regions = cat_cutouts.get_regions(self.regions_file)\n",
    "#             cat_misc.save_pickle(self.regions, self.regions_pickle_file)\n",
    "\n",
    "#     def get_cutouts(self):\n",
    "#         \"\"\"\n",
    "#         Generate cutouts for each HDU and save them as pickled files.\n",
    "#         \"\"\"\n",
    "#         names = ['hstha_hdu', 'musha_hdu', 'muscat_hdu']\n",
    "#         hdus_cutouts = {}\n",
    "\n",
    "#         for hdu, name in zip(self.hdus, names):\n",
    "#             print(f'[INFO] Running for {name}...')\n",
    "#             hdu_cutouts = cat_cutouts.get_croppeddata_all(hdu, self.regions)\n",
    "#             cat_misc.save_pickle(hdu_cutouts, f\"{self.cutout_dir}/{name}.pickel\")\n",
    "#             del hdu_cutouts\n",
    "#             _ = gc.collect()\n",
    "\n",
    "#         for name in names:\n",
    "#             hdu_cutouts = cat_misc.load_pickle(f\"{self.cutout_dir}/{name}.pickel\")\n",
    "#             hdus_cutouts[name] = hdu_cutouts\n",
    "\n",
    "#         cat_misc.save_pickle(hdus_cutouts, f\"{self.cutout_dir}/hdus_all.pickel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the class\n",
    "# catalog = PyHSTHACat(galaxy='ic5332')\n",
    "\n",
    "# # Create necessary directories\n",
    "# catalog.make_paths()\n",
    "\n",
    "# # Load and preprocess FITS files\n",
    "# catalog.load_files()\n",
    "\n",
    "# # Retrieve regions and sample table\n",
    "# catalog.get_regions_and_sample_table()\n",
    "\n",
    "# # Generate and save cutouts\n",
    "# catalog.get_cutouts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imports import *\n",
    "\n",
    "# class PyHSTHACat:\n",
    "#     def __init__(self, galaxy, galaxy_hst=None):\n",
    "#         \"\"\"\n",
    "#         Initialize the PyHSTHACat class with default parameters and file paths.\n",
    "#         \"\"\"\n",
    "#         # Define galaxy names\n",
    "#         self.galaxy = galaxy\n",
    "#         if galaxy_hst is None:\n",
    "#             self.galaxy_hst = self.galaxy\n",
    "#         else:\n",
    "#             self.galaxy_hst = galaxy_hst\n",
    "\n",
    "#         # Define root directory\n",
    "#         self.rootdir = '/Users/abarnes/Dropbox/work/Smallprojects/galaxies'\n",
    "        \n",
    "#         # Define filenames\n",
    "#         self.hstha_file = f\"{self.rootdir}/data_hstha/{self.galaxy_hst}/hst_contsub/{self.galaxy_hst}_hst_ha.fits\"\n",
    "#         self.hstha_err_file = f\"{self.rootdir}/data_hstha/{self.galaxy_hst}/hst_contsub/{self.galaxy_hst}_hst_ha_err.fits\"\n",
    "#         self.muscat_file = f\"{self.rootdir}/data_hstha/{self.galaxy_hst}/muse/{self.galaxy.upper()}_nebmask.fits\"\n",
    "#         musha_glob = f\"{self.rootdir}/data_hstha/{self.galaxy_hst}/muse/{self.galaxy.upper()}-*_MAPS.fits\"\n",
    "#         self.musha_file = glob(musha_glob)[0]  # Use glob to find the correct file\n",
    "\n",
    "#         # Define directories for outputs\n",
    "#         self.cutout_dir = f\"{self.rootdir}/data_hstha_nebulae_catalogue/{self.galaxy_hst}/cutouts\"\n",
    "#         self.catalogue_dir = f\"{self.rootdir}/data_hstha_nebulae_catalogue/{self.galaxy_hst}/catalogue\"\n",
    "#         self.cutouts_hdus_dir = f\"{self.rootdir}/data_hstha_nebulae_catalogue/{self.galaxy_hst}/cutouts_hdus\"\n",
    "\n",
    "#         # Flags for rerunning certain steps\n",
    "#         self.rerun_all = False\n",
    "#         self.rerun_regions = False\n",
    "#         self.rerun_masking = False\n",
    "#         self.rerun_cutouts = False\n",
    "\n",
    "#         # Define additional files\n",
    "#         self.regions_file = f\"{self.cutout_dir}/sample.reg\"\n",
    "#         self.regions_pickle_file = f\"{self.cutout_dir}/sample.pickel\"\n",
    "#         self.sample_table_file = f\"{self.rootdir}/data_misc/sample_table/phangs_sample_table_v1p6.fits\"\n",
    "#         self.muscat_table_file = f\"{self.rootdir}/data_misc/Nebulae_catalogue_v3/Nebulae_catalogue_v3.fits\"\n",
    "\n",
    "#         # In-memory placeholders (assigned in methods below)\n",
    "#         self.hstha_hdu = None\n",
    "#         self.hstha_err_hdu = None\n",
    "#         self.musha_hdu = None\n",
    "#         self.muscat_hdu = None\n",
    "#         self.hdus = []\n",
    "#         self.regions = None\n",
    "#         self.hstha_err = None\n",
    "#         self.props_all_final = None \n",
    "\n",
    "#     def make_paths(self):\n",
    "#         \"\"\"\n",
    "#         Create necessary directories for outputs. \n",
    "#         Deletes and recreates root directory if `rerun_all` is True.\n",
    "#         \"\"\"\n",
    "#         root_dir = f\"{self.rootdir}/data_hstha_nebulae_catalogue/{self.galaxy_hst}/\"\n",
    "#         print('[Info] Outputting to the following:')\n",
    "#         print(root_dir)\n",
    "\n",
    "#         # Remove and recreate root directory if rerun_all is set\n",
    "#         if self.rerun_all:\n",
    "#             os.system(f'rm -rf {root_dir}')\n",
    "\n",
    "#         # Create root directory if it doesn't exist\n",
    "#         if not os.path.isdir(root_dir):\n",
    "#             os.mkdir(root_dir)\n",
    "\n",
    "#         # Create subdirectories for cutouts, catalogues, and HDUs\n",
    "#         for path in [self.cutout_dir, self.catalogue_dir, self.cutouts_hdus_dir]:\n",
    "#             if not os.path.isdir(path):\n",
    "#                 os.mkdir(path)\n",
    "\n",
    "#     def load_files(self):\n",
    "#         \"\"\"\n",
    "#         Load FITS files and preprocess their data by handling NaNs and converting to float32 format.\n",
    "#         \"\"\"\n",
    "#         # Load primary HST H-alpha, HST H-alpha error, MUSE mask\n",
    "#         self.hstha_hdu = fits.open(self.hstha_file)[0]\n",
    "#         self.hstha_err_hdu = fits.open(self.hstha_err_file)[0]  # newly added\n",
    "#         self.musha_hdu = fits.open(self.musha_file)['HA6562_FLUX']\n",
    "#         self.muscat_hdu = fits.open(self.muscat_file)[0]\n",
    "\n",
    "#         # Update arrays to handle NaNs and ensure correct data type\n",
    "#         # For MUSE H-alpha FLUX\n",
    "#         self.musha_hdu.data[np.isnan(self.musha_hdu.data)] = -100\n",
    "        \n",
    "#         # For MUSE nebmask\n",
    "#         self.muscat_hdu.data = np.array(self.muscat_hdu.data, dtype=float)\n",
    "#         self.muscat_hdu.data[self.muscat_hdu.data == -1] = np.nan\n",
    "\n",
    "#         # Convert all HDUs to float32 format for compatibility\n",
    "#         hdus = [self.hstha_hdu, self.musha_hdu, self.muscat_hdu, self.hstha_err_hdu]\n",
    "#         hdus_converted = []\n",
    "#         for hdu in hdus:\n",
    "#             hdus_converted.append(cat_misc.convert_to_float32(hdu.copy()))\n",
    "\n",
    "#         # Reassign them to class attributes in the same order\n",
    "#         (\n",
    "#             self.hstha_hdu, \n",
    "#             self.musha_hdu, \n",
    "#             self.muscat_hdu,\n",
    "#             self.hstha_err_hdu\n",
    "#         ) = hdus_converted\n",
    "\n",
    "#         # Load the sample and MUSE tables\n",
    "#         self.muscat_table = cat_misc.get_museprops(self.galaxy, self.muscat_table_file)\n",
    "#         self.sample_table = cat_misc.get_galaxyprops(self.galaxy, self.sample_table_file)\n",
    "\n",
    "#     def get_regions(self):\n",
    "#         \"\"\"\n",
    "#         Retrieve regions and sample table. Uses existing region files if available unless rerun is flagged.\n",
    "#         \"\"\"\n",
    "#         if os.path.exists(self.regions_file) and not self.rerun_regions:\n",
    "#             print(f'[INFO] Using existing region file: {self.regions_file}')\n",
    "#             self.regions = cat_misc.load_pickle(self.regions_pickle_file)\n",
    "#         else:\n",
    "#             # Generate regions from the sample table\n",
    "#             print('[INFO] Generating new DS9 region file...')\n",
    "#             cat_cutouts.get_ds9regions_all(self.muscat_table, outputfile=self.regions_file)\n",
    "#             self.regions = cat_cutouts.get_regions(self.regions_file)\n",
    "#             cat_misc.save_pickle(self.regions, self.regions_pickle_file)\n",
    "\n",
    "#     def get_cutouts(self):\n",
    "#         \"\"\"\n",
    "#         Generate cutouts for each HDU (HST HA, MUSE HA, MUSE nebmask)\n",
    "#         and save them as pickled files. Also combine them into one dictionary.\n",
    "#         \"\"\"\n",
    "#         names = ['hstha_hdu', 'musha_hdu', 'muscat_hdu']\n",
    "#         self.hdus_cutouts = {}\n",
    "\n",
    "#         combined_path = f\"{self.cutout_dir}/hdus_all.pickel\"\n",
    "#         if os.path.exists(combined_path) and not self.rerun_cutouts:\n",
    "#             print(f'[INFO] All cutouts already exist. Loading from file {combined_path}')\n",
    "#             self.hdus_cutouts = cat_misc.load_pickle(combined_path)\n",
    "\n",
    "#         else: \n",
    "#             for hdu, name in zip([self.hstha_hdu, self.musha_hdu, self.muscat_hdu], names):\n",
    "#                 print(f'[INFO] Generating cutouts for {name}...')\n",
    "#                 hdu_cutouts = cat_cutouts.get_croppeddata_all(hdu, self.regions)\n",
    "#                 pickle_path = f\"{self.cutout_dir}/{name}.pickel\"\n",
    "#                 cat_misc.save_pickle(hdu_cutouts, pickle_path)\n",
    "#                 del hdu_cutouts\n",
    "#                 gc.collect()\n",
    "\n",
    "#             # Reload everything into a single dictionary\n",
    "#             for name in names:\n",
    "#                 pickle_path = f\"{self.cutout_dir}/{name}.pickel\"\n",
    "#                 self.hdus_cutouts[name] = cat_misc.load_pickle(pickle_path)\n",
    "\n",
    "#             # Store one final combined pickled dictionary\n",
    "#             print(f'[INFO] All HDU cutouts saved to {combined_path}')\n",
    "#             cat_misc.save_pickle(self.hdus_cutouts, combined_path) \n",
    "\n",
    "#     def make_catalogue(self):\n",
    "#         \"\"\"\n",
    "#         Make a final catalogue by:\n",
    "#           - Checking directory creation,\n",
    "#           - Interpolating masks,\n",
    "#           - Computing noise properties,\n",
    "#           - Loading region-based cutouts,\n",
    "#           - Masking data with threshold and pruning,\n",
    "#           - Computing dendrogram-based properties,\n",
    "#           - Merging with MUSE tables,\n",
    "#           - Correcting fluxes/luminosities,\n",
    "#           - And saving outputs (catalog and masks).\n",
    "#         \"\"\"\n",
    "#         # Print the file paths for debugging\n",
    "#         print(\"[CATALOGUE] Running catalogue generation for:\", self.galaxy)\n",
    "\n",
    "#         # Check or create the output directory\n",
    "#         cat_misc.checkmakedir(self.catalogue_dir)\n",
    "\n",
    "#         # Noise from the error map\n",
    "#         if self.hstha_err is None:\n",
    "#             self.hstha_err = np.nanmedian(self.hstha_err_hdu.data)\n",
    "\n",
    "#         # Load cutout hdus with masked data (if exists) or generate them\n",
    "#         hdus_file = f'{self.cutout_dir}/hdus_all_withmasked.pickel'\n",
    "#         if os.path.exists(hdus_file) and (not self.rerun_masking):\n",
    "#             muscat_regionIDs = self.muscat_table['region_ID']\n",
    "#             hdus = cat_misc.load_pickle(hdus_file)\n",
    "#         else:\n",
    "#             muscat_regionIDs = self.muscat_table['region_ID']\n",
    "#             hdus = cat_mask.get_maskedhdus(self.hdus_cutouts, self.regions, muscat_regionIDs)\n",
    "#             cat_misc.save_pickle(hdus, hdus_file)\n",
    "\n",
    "#         props_all       = []\n",
    "#         hdus_mask       = []\n",
    "#         hdus_mask_id    = []\n",
    "#         hdus_data_masked= []\n",
    "\n",
    "#         # Main loop over region IDs\n",
    "#         for i in tqdm(range(len(muscat_regionIDs)), desc='Get sources:', position=0):\n",
    "#             regionID = np.int16(muscat_regionIDs[i])\n",
    "            \n",
    "#             data   = hdus['hstha_hdu_masked'][i].data.copy()\n",
    "#             header = hdus['hstha_hdu_masked'][i].header.copy()\n",
    "\n",
    "#             # Create threshold-based masks\n",
    "#             mask_low       = cat_mask.get_threshmask(data, self.hstha_err, thresh=1)\n",
    "#             mask_low_prune = cat_mask.get_prunemask(mask_low, thresh=50)\n",
    "#             mask_high      = cat_mask.get_threshmask(data, self.hstha_err, thresh=3)\n",
    "#             mask_grow      = ndimage.binary_dilation(mask_high, iterations=-1, mask=mask_low_prune)\n",
    "#             mask_prune     = cat_mask.get_prunemask(mask_grow, thresh=9)\n",
    "#             mask_filled    = cat_mask.get_filled_outer_contour(mask_prune)\n",
    "#             mask_final     = mask_filled.copy()\n",
    "\n",
    "#             # Save the mask\n",
    "#             hdu_mask = fits.PrimaryHDU(mask_final.astype(np.float32), header)\n",
    "#             hdus_mask.append(hdu_mask)\n",
    "\n",
    "#             # Create ID-based mask\n",
    "#             hdu_mask_id = hdu_mask.copy()\n",
    "#             hdu_mask_id.data = (hdu_mask_id.data * i).astype(np.float32)\n",
    "#             hdu_mask_id.data[~mask_final] = -1\n",
    "#             hdus_mask_id.append(hdu_mask_id)\n",
    "\n",
    "#             # Mask the data itself\n",
    "#             data_masked = data.copy()\n",
    "#             data_masked[~mask_final] = np.nan\n",
    "#             hdu_data_masked = fits.PrimaryHDU(data_masked, header)\n",
    "#             hdus_data_masked.append(hdu_data_masked)\n",
    "\n",
    "#             # If everything is masked, skip\n",
    "#             if np.nansum(~np.isnan(data_masked)) == 0:\n",
    "#                 continue\n",
    "\n",
    "#             # Pixel scale\n",
    "#             try:\n",
    "#                 pixsize = np.array([abs(header['CDELT1']), abs(header['CDELT2'])]).mean() * au.degree\n",
    "#             except:\n",
    "#                 pixsize = np.array([abs(header['CD1_1']), abs(header['CD2_2'])]).mean() * au.degree\n",
    "\n",
    "#             if pixsize.value == 1:\n",
    "#                 if 'CD1_1' in header and 'CD2_2' in header:\n",
    "#                     pixsize = np.array([abs(header['CD1_1']), abs(header['CD2_2'])]).mean() * au.degree\n",
    "#                 elif 'PC1_1' in header and 'PC2_2' in header:\n",
    "#                     pixsize = np.array([abs(header['PC1_1']), abs(header['PC2_2'])]).mean() * au.degree\n",
    "\n",
    "#             npix      = np.nansum(mask_final) * au.pix\n",
    "#             flux      = np.nansum(data_masked) * au.erg/au.s/au.cm**2\n",
    "#             flux_err  = np.sqrt(npix.value)*self.hstha_err *flux.unit\n",
    "\n",
    "#             area_exact  = npix.value*np.abs(pixsize.to(au.arcsec)**2)\n",
    "#             radius_circ = np.sqrt(area_exact/np.pi).to('arcsec')\n",
    "\n",
    "#             flux_max  = np.nanmax(data_masked)*flux.unit\n",
    "#             flux_min  = np.nanmin(data_masked)*flux.unit\n",
    "#             flux_mean = np.nanmean(data_masked)*flux.unit\n",
    "\n",
    "#             x_max, y_max = np.where(data_masked == flux_max.value)\n",
    "#             if len(x_max)>1 or len(y_max)>1:\n",
    "#                 x_max, y_max = np.nanmean(x_max), np.nanmean(y_max)\n",
    "#             else:\n",
    "#                 x_max, y_max = x_max[0], y_max[0]\n",
    "\n",
    "#             data_zeros = data_masked.copy()\n",
    "#             data_zeros[np.isnan(data_zeros)] = 0\n",
    "#             x_com, y_com = ndimage.center_of_mass(data_zeros)\n",
    "\n",
    "#             wcs = WCS(header)\n",
    "#             ra_max, dec_max = wcs.array_index_to_world_values([[y_max, x_max]])[0] * au.deg\n",
    "#             ra_com, dec_com = wcs.array_index_to_world_values([[y_com, x_com]])[0] * au.deg\n",
    "\n",
    "#             table_data = [\n",
    "#                 regionID, x_max, y_max, x_com, y_com, \n",
    "#                 ra_max, dec_max, ra_com, dec_com,\n",
    "#                 npix, flux, flux_err, area_exact, radius_circ, flux_max, flux_min, flux_mean\n",
    "#             ]\n",
    "#             table_data = [np.array(td) for td in table_data]\n",
    "#             table_names = [\n",
    "#                 'region_ID', 'x_max', 'y_max', 'x_com', 'y_com', \n",
    "#                 'ra_max', 'dec_max', 'ra_com', 'dec_com',\n",
    "#                 'npix', 'flux', 'flux_err', 'area_exact', 'radius_circ', \n",
    "#                 'flux_max', 'flux_min', 'flux_mean'\n",
    "#             ]\n",
    "\n",
    "#             props = QTable(np.array(table_data), names=table_names)\n",
    "#             props['x_max'].unit       = au.pix\n",
    "#             props['y_max'].unit       = au.pix\n",
    "#             props['x_com'].unit       = au.pix\n",
    "#             props['y_com'].unit       = au.pix\n",
    "#             props['ra_max'].unit      = au.deg\n",
    "#             props['dec_max'].unit     = au.deg\n",
    "#             props['ra_com'].unit      = au.deg\n",
    "#             props['dec_com'].unit     = au.deg\n",
    "#             props['npix'].unit        = au.pix\n",
    "#             props['flux'].unit        = flux.unit\n",
    "#             props['flux_err'].unit    = flux.unit\n",
    "#             props['area_exact'].unit  = au.arcsec**2\n",
    "#             props['radius_circ'].unit = au.arcsec\n",
    "#             props['flux_max'].unit    = flux_max.unit\n",
    "#             props['flux_min'].unit    = flux_min.unit\n",
    "#             props['flux_mean'].unit   = flux_mean.unit\n",
    "\n",
    "#             # Convert radius_circ to parsecs\n",
    "#             pcperarcsec = cat_props.get_pcperarcsec(self.sample_table)\n",
    "#             props['radius_circ_pc'] = props['radius_circ']*pcperarcsec\n",
    "\n",
    "#             # Dendrogram analysis (very lenient params)\n",
    "#             # data_masked_zero = data_masked.copy()\n",
    "#             # data_masked_zero[np.isnan(data_masked_zero)] = 0\n",
    "\n",
    "#             dendro = Dendrogram.compute(\n",
    "#                 data_masked, \n",
    "#                 min_delta=-1000, \n",
    "#                 min_value=0, \n",
    "#                 min_npix=0, \n",
    "#                 wcs=wcs\n",
    "#             )\n",
    "#             metadata = {\n",
    "#                 'data_unit': au.Jy/au.beam,\n",
    "#                 'spatial_scale': pixsize.to('arcsec'),\n",
    "#                 'beam_major': 0.1*au.arcsec,\n",
    "#                 'beam_minor': 0.1*au.arcsec\n",
    "#             }\n",
    "\n",
    "#             props_dendro = pp_catalog(dendro.trunk, metadata, verbose=False)  \n",
    "#             props_dendro = QTable(props_dendro)\n",
    "#             props_dendro = props_dendro[np.nanargmax(props_dendro['flux'])]\n",
    "\n",
    "#             ra_dendro, dec_dendro = wcs.array_index_to_world_values(\n",
    "#                 [[props_dendro['x_cen'].value, props_dendro['y_cen'].value]]\n",
    "#             )[0] * au.deg\n",
    "\n",
    "#             props['x_mom']          = props_dendro['x_cen']\n",
    "#             props['y_mom']          = props_dendro['y_cen']\n",
    "#             props['ra_mom']         = ra_dendro\n",
    "#             props['dec_mom']        = dec_dendro\n",
    "#             props['area_ellipse']   = props_dendro['area_ellipse']\n",
    "#             props['major_sigma']    = props_dendro['major_sigma']\n",
    "#             props['minor_sigma']    = props_dendro['minor_sigma']\n",
    "#             props['mean_sigma']     = props_dendro['radius']\n",
    "#             props['position_angle'] = props_dendro['position_angle']\n",
    "#             props['mean_sigma_pc']  = props['mean_sigma'] * pcperarcsec\n",
    "\n",
    "#             # Dendrogram complexity with higher thresholds\n",
    "#             dendro = Dendrogram.compute(\n",
    "#                 data_masked,\n",
    "#                 min_delta=(self.hstha_err*3),\n",
    "#                 min_value=(self.hstha_err),\n",
    "#                 min_npix=9,\n",
    "#                 wcs=wcs\n",
    "#             )\n",
    "#             dendro_IDs = np.unique(dendro.index_map.data)\n",
    "#             dendro_IDs = [dID for dID in dendro_IDs if dID > -1]\n",
    "\n",
    "#             dendro_complex = len(dendro_IDs)\n",
    "#             dendro_complex_leaves = len(dendro.leaves)\n",
    "#             complexity_rms = np.sqrt(np.nanmean((data_masked.copy())**2))\n",
    "#             complexity_std = np.nanstd(data_masked.copy())\n",
    "\n",
    "#             props['complexity_score'] = dendro_complex\n",
    "#             props['complexity_score_leaves'] = dendro_complex_leaves\n",
    "#             props['complexity_rms'] = complexity_rms\n",
    "#             props['complexity_std'] = complexity_std\n",
    "\n",
    "#             # Edge & Touch flags\n",
    "#             flag_edge = np.isnan(hdus['hstha_hdu_masked_ones'][i].data).any()*1\n",
    "#             props.add_column(Column(flag_edge, name='flag_edge_hst'))\n",
    "\n",
    "#             mask_touch = (ndimage.binary_dilation(mask_final) & ~mask_final)\n",
    "#             flag_touch = (np.nansum(np.isnan(data[mask_touch])) > 0)*1\n",
    "#             props.add_column(Column(flag_touch, name='flag_touch_hst'))\n",
    "\n",
    "#             props_all.append(props)\n",
    "\n",
    "#         # Stack all region properties\n",
    "#         props_all = vstack(props_all)\n",
    "\n",
    "#         # Merge with MUSE data (region_ID-based)\n",
    "#         self.muscat_table = cat_misc.get_museprops(self.galaxy, self.muscat_table_file)\n",
    "#         self.muscat_table_rename = self.muscat_table.copy()\n",
    "#         for column in self.muscat_table_rename.colnames:\n",
    "#             self.muscat_table_rename.rename_column(column, column+'_MUSE')\n",
    "#         self.muscat_table_rename.rename_column('gal_name_MUSE', 'gal_name')\n",
    "#         self.muscat_table_rename.rename_column('region_ID_MUSE', 'region_ID')\n",
    "#         self.muscat_table_rename.rename_column('Lum_HA6562_CORR_MUSE', 'HA6562_LUMINOSITY_MUSE')\n",
    "\n",
    "#         props_all_final = join(props_all, self.muscat_table_rename, keys='region_ID')\n",
    "\n",
    "#         # Correct fluxes & luminosities\n",
    "#         props_all_final['flux_corr'] = cat_props.correct_ha_flux(\n",
    "#             props_all_final, props_all_final['flux']\n",
    "#         )\n",
    "#         props_all_final['flux_err_corr'] = cat_props.correct_ha_flux(\n",
    "#             props_all_final, props_all_final['flux_err']\n",
    "#         )\n",
    "#         dist_val = self.sample_table['dist'][0]  \n",
    "#         props_all_final['lum_hst'] = cat_props.calculate_luminosity(\n",
    "#             props_all_final['flux_corr']*1e-20, dist_val\n",
    "#         )\n",
    "#         props_all_final['lum_err_hst'] = cat_props.calculate_luminosity(\n",
    "#             props_all_final['flux_err_corr']*1e-20, dist_val\n",
    "#         )\n",
    "#         props_all_final['region_circ_rad_pc_MUSE'] = cat_props.calculate_radius(\n",
    "#             props_all_final['region_circ_rad_MUSE'], dist_val\n",
    "#         )\n",
    "\n",
    "#         # Rename columns\n",
    "#         props_all_final.rename_column('flux',          'HA6562_FLUX_HST')\n",
    "#         props_all_final.rename_column('flux_err',      'HA6562_FLUX_ERR_HST')\n",
    "#         props_all_final.rename_column('flux_corr',     'HA6562_FLUX_CORR_HST')\n",
    "#         props_all_final.rename_column('flux_err_corr', 'HA6562_FLUX_ERR_CORR_HST')\n",
    "#         props_all_final.rename_column('lum_hst',       'HA6562_LUMINOSITY_HST')\n",
    "#         props_all_final.rename_column('lum_err_hst',   'HA6562_LUMINOSITY_ERR_HST')\n",
    "\n",
    "#         self.props_all_final = props_all_final\n",
    "\n",
    "#         # Save final table\n",
    "#         props_file = f'{self.catalogue_dir}/props_all.fits'\n",
    "#         props_all_final.write(props_file, overwrite=True)\n",
    "#         print(f'[CATALOGUE] Final properties table saved to: {props_file}')\n",
    "\n",
    "#         # Save masks\n",
    "#         mask_file = f'{self.catalogue_dir}/{self.galaxy}_mask.fits'\n",
    "#         cat_mask.get_hdumask(self.hstha_hdu, hdus_mask_id, outputfile=mask_file)\n",
    "\n",
    "#         complexity_file = f'{self.catalogue_dir}/{self.galaxy}_complexity.fits'\n",
    "#         cat_mask.get_hducomplex(props_all_final, inputfile=mask_file,\n",
    "#                                 outputfile=complexity_file)\n",
    "\n",
    "#         regions_out = f'{self.catalogue_dir}/{self.galaxy}_mask'\n",
    "#         cat_mask.get_ds9regions(props_all, outputfile=regions_out)\n",
    "\n",
    "#         print(f'[CATALOGUE] Masks saved to {mask_file} and {complexity_file}.')\n",
    "#         print(f'[CATALOGUE] DS9 regions output to {regions_out}.')\n",
    "#         print('[CATALOGUE] Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the class\n",
    "# catalog = PyHSTHACat(galaxy='ic5332')\n",
    "\n",
    "# # Create necessary directories\n",
    "# catalog.make_paths()\n",
    "\n",
    "# # Load and preprocess FITS files\n",
    "# catalog.load_files()\n",
    "\n",
    "# # Retrieve regions and sample table\n",
    "# catalog.get_regions()\n",
    "\n",
    "# # Generate and save cutouts\n",
    "# catalog.get_cutouts()\n",
    "\n",
    "# # Make the catalogue\n",
    "# catalog.make_catalogue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imports import *\n",
    "\n",
    "# class PyHSTHACat:\n",
    "#     def __init__(self, galaxy, galaxy_hst=None):\n",
    "#         \"\"\"\n",
    "#         Initialize the PyHSTHACat class with default parameters and file paths.\n",
    "#         \"\"\"\n",
    "#         # -------------------------------------------------------\n",
    "#         # Basic galaxy naming and root directory\n",
    "#         # -------------------------------------------------------\n",
    "#         self.galaxy = galaxy\n",
    "#         self.galaxy_hst = galaxy_hst if galaxy_hst else galaxy\n",
    "#         self.galaxy_cat = self.galaxy  # e.g. used for association file naming\n",
    "#         self.rootdir = '/Users/abarnes/Dropbox/work/Smallprojects/galaxies'\n",
    "\n",
    "#         # -------------------------------------------------------\n",
    "#         # Default file paths for HST/MUSE data\n",
    "#         # -------------------------------------------------------\n",
    "#         self.hstha_file = f\"{self.rootdir}/data_hstha/{self.galaxy_hst}/hst_contsub/{self.galaxy_hst}_hst_ha.fits\"\n",
    "#         self.hstha_err_file = f\"{self.rootdir}/data_hstha/{self.galaxy_hst}/hst_contsub/{self.galaxy_hst}_hst_ha_err.fits\"\n",
    "#         self.muscat_file = f\"{self.rootdir}/data_hstha/{self.galaxy_hst}/muse/{self.galaxy.upper()}_nebmask.fits\"\n",
    "\n",
    "#         musha_glob = f\"{self.rootdir}/data_hstha/{self.galaxy_hst}/muse/{self.galaxy.upper()}-*_MAPS.fits\"\n",
    "#         self.musha_file = glob(musha_glob)[0]  # Use glob to find the correct file\n",
    "\n",
    "#         # -------------------------------------------------------\n",
    "#         # Output directories\n",
    "#         # -------------------------------------------------------\n",
    "#         self.cutout_dir = f\"{self.rootdir}/data_hstha_nebulae_catalogue/{self.galaxy_hst}/cutouts\"\n",
    "#         self.catalogue_dir = f\"{self.rootdir}/data_hstha_nebulae_catalogue/{self.galaxy_hst}/catalogue\"\n",
    "#         self.cutouts_hdus_dir = f\"{self.rootdir}/data_hstha_nebulae_catalogue/{self.galaxy_hst}/cutouts_hdus\"\n",
    "\n",
    "#         # -------------------------------------------------------\n",
    "#         # Rerun flags\n",
    "#         # -------------------------------------------------------\n",
    "#         self.rerun_all = False\n",
    "#         self.rerun_regions = False\n",
    "#         self.rerun_masking = False\n",
    "#         self.rerun_cutouts = False\n",
    "#         self.rerun_cutouts_associations = False\n",
    "\n",
    "#         # -------------------------------------------------------\n",
    "#         # Ancillary files: Regions, tables\n",
    "#         # -------------------------------------------------------\n",
    "#         self.regions_file = f\"{self.cutout_dir}/sample.reg\"\n",
    "#         self.regions_pickle_file = f\"{self.cutout_dir}/sample.pickel\"\n",
    "#         self.sample_table_file = f\"{self.rootdir}/data_misc/sample_table/phangs_sample_table_v1p6.fits\"\n",
    "#         self.muscat_table_file = f\"{self.rootdir}/data_misc/Nebulae_catalogue_v3/Nebulae_catalogue_v3.fits\"\n",
    "\n",
    "#         # -------------------------------------------------------\n",
    "#         # Placeholders for loaded HDUs and tables\n",
    "#         # -------------------------------------------------------\n",
    "#         self.hstha_hdu = None\n",
    "#         self.hstha_err_hdu = None\n",
    "#         self.musha_hdu = None\n",
    "#         self.muscat_hdu = None\n",
    "#         self.hdus = []\n",
    "#         self.regions = None\n",
    "#         self.hstha_err = None\n",
    "#         self.props = None\n",
    "        \n",
    "#         self.muscat_table = None\n",
    "#         self.sample_table = None\n",
    "\n",
    "#         self.props = None\n",
    "#         self.props_associations = None\n",
    "\n",
    "#         # -------------------------------------------------------\n",
    "#         # Stellar association files (from the PHANGS-HST project)\n",
    "#         # -------------------------------------------------------\n",
    "#         # \"v\" stands for \"visually selected\" associations, \"nuv\" for \"NUV-selected\"\n",
    "#         # \"08pc\", \"16pc\", \"32pc\", \"64pc\" are different physical scale windows.\n",
    "\n",
    "#         # For convenience, define a short helper:\n",
    "#         def assoc_path(fmt, scale, selection):\n",
    "#             \"\"\"\n",
    "#             Returns the absolute path for a given scale (08pc, etc.)\n",
    "#             and selection type (vselect or nuvselect).\n",
    "#             Example usage:\n",
    "#                 assoc_path('table', '08pc', 'v')\n",
    "#                 assoc_path('mask',  '32pc', 'nuv')\n",
    "#             \"\"\"\n",
    "#             if fmt == 'table':\n",
    "#                 # Example: /data_misc/multiscale_stellar_associations/ic5332/vselect/ws8pc/...main.fits\n",
    "#                 return (\n",
    "#                     f\"{self.rootdir}/data_misc/multiscale_stellar_associations/\"\n",
    "#                     f\"{self.galaxy_cat}/{selection}select/ws{scale}/\"\n",
    "#                     f\"PHANGS_IR4_hst_wfc3_{self.galaxy_cat}_v1p3_multi_assoc-{selection}select-ws{scale}-main.fits\"\n",
    "#                 )\n",
    "#             elif fmt == 'mask':\n",
    "#                 # Example: /data_misc/multiscale_stellar_associations/ic5332/vselect/ws8pc/...idmask.fits\n",
    "#                 return (\n",
    "#                     f\"{self.rootdir}/data_misc/multiscale_stellar_associations/\"\n",
    "#                     f\"{self.galaxy_cat}/{selection}select/ws{scale}/\"\n",
    "#                     f\"PHANGS_IR4_hst_wfc3_{self.galaxy_cat}_v1p3_multi_assoc-{selection}select-ws{scale}-idmask.fits\"\n",
    "#                 )\n",
    "#             else:\n",
    "#                 raise ValueError(\"Unknown fmt. Use 'table' or 'mask'.\")\n",
    "\n",
    "#         # You can expand these if needed\n",
    "#         self.association_table_file_v_08pc   = assoc_path('table', '8pc',  'v')\n",
    "#         self.association_table_file_v_16pc   = assoc_path('table', '16pc', 'v')\n",
    "#         self.association_table_file_v_32pc   = assoc_path('table', '32pc', 'v')\n",
    "#         self.association_table_file_v_64pc   = assoc_path('table', '64pc', 'v')\n",
    "#         self.association_table_file_nuv_08pc = assoc_path('table', '8pc',  'nuv')\n",
    "#         self.association_table_file_nuv_16pc = assoc_path('table', '16pc', 'nuv')\n",
    "#         self.association_table_file_nuv_32pc = assoc_path('table', '32pc', 'nuv')\n",
    "#         self.association_table_file_nuv_64pc = assoc_path('table', '64pc', 'nuv')\n",
    "\n",
    "#         self.association_mask_file_v_08pc   = assoc_path('mask', '8pc',  'v')\n",
    "#         self.association_mask_file_v_16pc   = assoc_path('mask', '16pc', 'v')\n",
    "#         self.association_mask_file_v_32pc   = assoc_path('mask', '32pc', 'v')\n",
    "#         self.association_mask_file_v_64pc   = assoc_path('mask', '64pc', 'v')\n",
    "#         self.association_mask_file_nuv_08pc = assoc_path('mask', '8pc',  'nuv')\n",
    "#         self.association_mask_file_nuv_16pc = assoc_path('mask', '16pc', 'nuv')\n",
    "#         self.association_mask_file_nuv_32pc = assoc_path('mask', '32pc', 'nuv')\n",
    "#         self.association_mask_file_nuv_64pc = assoc_path('mask', '64pc', 'nuv')\n",
    "\n",
    "#         # This is the mask for the final nebula catalog (H-alpha objects)\n",
    "#         self.catalog_mask_file = f\"{self.catalogue_dir}/{self.galaxy}_mask.fits\"\n",
    "\n",
    "#     def make_paths(self):\n",
    "#         \"\"\"\n",
    "#         Create necessary directories for outputs. \n",
    "#         Deletes and recreates root directory if `rerun_all` is True.\n",
    "#         \"\"\"\n",
    "#         root_dir = f\"{self.rootdir}/data_hstha_nebulae_catalogue/{self.galaxy_hst}/\"\n",
    "#         print('[Info] Outputting to the following:')\n",
    "#         print(root_dir)\n",
    "\n",
    "#         # Remove and recreate root directory if rerun_all is set\n",
    "#         if self.rerun_all:\n",
    "#             os.system(f'rm -rf {root_dir}')\n",
    "\n",
    "#         # Create root directory if it doesn't exist\n",
    "#         if not os.path.isdir(root_dir):\n",
    "#             os.mkdir(root_dir)\n",
    "\n",
    "#         # Create subdirectories for cutouts, catalogues, and HDUs\n",
    "#         for path in [self.cutout_dir, self.catalogue_dir, self.cutouts_hdus_dir]:\n",
    "#             if not os.path.isdir(path):\n",
    "#                 os.mkdir(path)\n",
    "\n",
    "#     def load_files(self):\n",
    "#         \"\"\"\n",
    "#         Load FITS files and preprocess their data by handling NaNs and converting to float32 format.\n",
    "#         Also loads the sample and MUSE property tables.\n",
    "#         \"\"\"\n",
    "#         # Load primary HST H-alpha, HST H-alpha error, MUSE mask\n",
    "#         self.hstha_hdu     = fits.open(self.hstha_file)[0]\n",
    "#         self.hstha_err_hdu = fits.open(self.hstha_err_file)[0]\n",
    "#         self.musha_hdu     = fits.open(self.musha_file)['HA6562_FLUX']\n",
    "#         self.muscat_hdu    = fits.open(self.muscat_file)[0]\n",
    "\n",
    "#         # Update arrays to handle NaNs and ensure correct data type\n",
    "#         # For MUSE H-alpha FLUX\n",
    "#         self.musha_hdu.data[np.isnan(self.musha_hdu.data)] = -100\n",
    "        \n",
    "#         # For MUSE nebmask\n",
    "#         self.muscat_hdu.data = np.array(self.muscat_hdu.data, dtype=float)\n",
    "#         self.muscat_hdu.data[self.muscat_hdu.data == -1] = np.nan\n",
    "\n",
    "#         # Convert all HDUs to float32 format for compatibility\n",
    "#         hdus = [self.hstha_hdu, self.musha_hdu, self.muscat_hdu, self.hstha_err_hdu]\n",
    "#         hdus_converted = []\n",
    "#         for hdu in hdus:\n",
    "#             hdus_converted.append(cat_misc.convert_to_float32(hdu.copy()))\n",
    "\n",
    "#         self.hstha_hdu, self.musha_hdu, self.muscat_hdu, self.hstha_err_hdu = hdus_converted\n",
    "\n",
    "#         # Load the sample and MUSE tables\n",
    "#         self.muscat_table = cat_misc.get_museprops(self.galaxy, self.muscat_table_file)\n",
    "#         self.sample_table = cat_misc.get_galaxyprops(self.galaxy, self.sample_table_file)\n",
    "\n",
    "#     def get_regions(self):\n",
    "#         \"\"\"\n",
    "#         Retrieve DS9 regions from disk (if they exist) or create them \n",
    "#         from the MUSE table and store them as a pickle.\n",
    "#         \"\"\"\n",
    "#         if os.path.exists(self.regions_file) and not self.rerun_regions:\n",
    "#             print(f'[INFO] Using existing region file: {self.regions_file}')\n",
    "#             self.regions = cat_misc.load_pickle(self.regions_pickle_file)\n",
    "#         else:\n",
    "#             print('[INFO] Generating new DS9 region file...')\n",
    "#             cat_cutouts.get_ds9regions_all(self.muscat_table, outputfile=self.regions_file)\n",
    "#             self.regions = cat_cutouts.get_regions(self.regions_file)\n",
    "#             cat_misc.save_pickle(self.regions, self.regions_pickle_file)\n",
    "\n",
    "#     def get_cutouts(self):\n",
    "#         \"\"\"\n",
    "#         Generate cutouts for each HDU and save them to disk. \n",
    "#         Loads them into a dictionary for quick reuse.\n",
    "#         \"\"\"\n",
    "#         names = ['hstha_hdu', 'musha_hdu', 'muscat_hdu']\n",
    "#         self.hdus_cutouts = {}\n",
    "\n",
    "#         combined_path = f\"{self.cutout_dir}/hdus_all.pickel\"\n",
    "#         if os.path.exists(combined_path) and not self.rerun_cutouts:\n",
    "#             print(f'[INFO] All cutouts already exist. Loading from file {combined_path}')\n",
    "#             self.hdus_cutouts = cat_misc.load_pickle(combined_path)\n",
    "#         else:\n",
    "#             # Generate cutouts and pickle them individually\n",
    "#             for hdu, name in zip([self.hstha_hdu, self.musha_hdu, self.muscat_hdu], names):\n",
    "#                 print(f'[INFO] Generating cutouts for {name}...')\n",
    "#                 hdu_cutouts = cat_cutouts.get_croppeddata_all(hdu, self.regions)\n",
    "#                 pickle_path = f\"{self.cutout_dir}/{name}.pickel\"\n",
    "#                 cat_misc.save_pickle(hdu_cutouts, pickle_path)\n",
    "#                 del hdu_cutouts\n",
    "#                 gc.collect()\n",
    "\n",
    "#             # Load them back into one dictionary\n",
    "#             for name in names:\n",
    "#                 pickle_path = f\"{self.cutout_dir}/{name}.pickel\"\n",
    "#                 self.hdus_cutouts[name] = cat_misc.load_pickle(pickle_path)\n",
    "\n",
    "#             # Store the combined dictionary\n",
    "#             print(f'[INFO] All HDU cutouts saved to {combined_path}')\n",
    "#             cat_misc.save_pickle(self.hdus_cutouts, combined_path)\n",
    "\n",
    "#     def make_catalogue(self):\n",
    "#         \"\"\"\n",
    "#         Make a final catalogue by:\n",
    "#           - Checking directory creation,\n",
    "#           - Interpolating masks,\n",
    "#           - Computing noise properties,\n",
    "#           - Loading region-based cutouts,\n",
    "#           - Masking data with threshold and pruning,\n",
    "#           - Computing dendrogram-based properties,\n",
    "#           - Merging with MUSE tables,\n",
    "#           - Correcting fluxes/luminosities,\n",
    "#           - And saving outputs (catalog and masks).\n",
    "#         \"\"\"\n",
    "#         # Print the file paths for debugging\n",
    "#         print(\"[CATALOGUE] Running catalogue generation for:\", self.galaxy)\n",
    "\n",
    "#         # Check or create the output directory\n",
    "#         cat_misc.checkmakedir(self.catalogue_dir)\n",
    "\n",
    "#         # Noise from the error map\n",
    "#         if self.hstha_err is None:\n",
    "#             self.hstha_err = np.nanmedian(self.hstha_err_hdu.data)\n",
    "\n",
    "#         # Load cutout hdus with masked data (if exists) or generate them\n",
    "#         hdus_file = f'{self.cutout_dir}/hdus_all_withmasked.pickel'\n",
    "#         if os.path.exists(hdus_file) and (not self.rerun_masking):\n",
    "#             muscat_regionIDs = self.muscat_table['region_ID']\n",
    "#             hdus = cat_misc.load_pickle(hdus_file)\n",
    "#         else:\n",
    "#             muscat_regionIDs = self.muscat_table['region_ID']\n",
    "#             hdus = cat_mask.get_maskedhdus(self.hdus_cutouts, self.regions, muscat_regionIDs)\n",
    "#             cat_misc.save_pickle(hdus, hdus_file)\n",
    "\n",
    "#         props_all       = []\n",
    "#         hdus_mask       = []\n",
    "#         hdus_mask_id    = []\n",
    "#         hdus_data_masked= []\n",
    "\n",
    "#         # Main loop over region IDs\n",
    "#         for i in tqdm(range(len(muscat_regionIDs)), desc='Get sources:', position=0):\n",
    "#             regionID = np.int16(muscat_regionIDs[i])\n",
    "            \n",
    "#             data   = hdus['hstha_hdu_masked'][i].data.copy()\n",
    "#             header = hdus['hstha_hdu_masked'][i].header.copy()\n",
    "\n",
    "#             # Create threshold-based masks\n",
    "#             mask_low       = cat_mask.get_threshmask(data, self.hstha_err, thresh=1)\n",
    "#             mask_low_prune = cat_mask.get_prunemask(mask_low, thresh=50)\n",
    "#             mask_high      = cat_mask.get_threshmask(data, self.hstha_err, thresh=3)\n",
    "#             mask_grow      = ndimage.binary_dilation(mask_high, iterations=-1, mask=mask_low_prune)\n",
    "#             mask_prune     = cat_mask.get_prunemask(mask_grow, thresh=9)\n",
    "#             mask_filled    = cat_mask.get_filled_outer_contour(mask_prune)\n",
    "#             mask_final     = mask_filled.copy()\n",
    "\n",
    "#             # Save the mask\n",
    "#             hdu_mask = fits.PrimaryHDU(mask_final.astype(np.float32), header)\n",
    "#             hdus_mask.append(hdu_mask)\n",
    "\n",
    "#             # Create ID-based mask\n",
    "#             hdu_mask_id = hdu_mask.copy()\n",
    "#             hdu_mask_id.data = (hdu_mask_id.data * i).astype(np.float32)\n",
    "#             hdu_mask_id.data[~mask_final] = -1\n",
    "#             hdus_mask_id.append(hdu_mask_id)\n",
    "\n",
    "#             # Mask the data itself\n",
    "#             data_masked = data.copy()\n",
    "#             data_masked[~mask_final] = np.nan\n",
    "#             hdu_data_masked = fits.PrimaryHDU(data_masked, header)\n",
    "#             hdus_data_masked.append(hdu_data_masked)\n",
    "\n",
    "#             # If everything is masked, skip\n",
    "#             if np.nansum(~np.isnan(data_masked)) == 0:\n",
    "#                 continue\n",
    "\n",
    "#             # Pixel scale\n",
    "#             try:\n",
    "#                 pixsize = np.array([abs(header['CDELT1']), abs(header['CDELT2'])]).mean() * au.degree\n",
    "#             except:\n",
    "#                 pixsize = np.array([abs(header['CD1_1']), abs(header['CD2_2'])]).mean() * au.degree\n",
    "\n",
    "#             if pixsize.value == 1:\n",
    "#                 if 'CD1_1' in header and 'CD2_2' in header:\n",
    "#                     pixsize = np.array([abs(header['CD1_1']), abs(header['CD2_2'])]).mean() * au.degree\n",
    "#                 elif 'PC1_1' in header and 'PC2_2' in header:\n",
    "#                     pixsize = np.array([abs(header['PC1_1']), abs(header['PC2_2'])]).mean() * au.degree\n",
    "\n",
    "#             npix      = np.nansum(mask_final) * au.pix\n",
    "#             flux      = np.nansum(data_masked) * au.erg/au.s/au.cm**2\n",
    "#             flux_err  = np.sqrt(npix.value)*self.hstha_err *flux.unit\n",
    "\n",
    "#             area_exact  = npix.value*np.abs(pixsize.to(au.arcsec)**2)\n",
    "#             radius_circ = np.sqrt(area_exact/np.pi).to('arcsec')\n",
    "\n",
    "#             flux_max  = np.nanmax(data_masked)*flux.unit\n",
    "#             flux_min  = np.nanmin(data_masked)*flux.unit\n",
    "#             flux_mean = np.nanmean(data_masked)*flux.unit\n",
    "\n",
    "#             x_max, y_max = np.where(data_masked == flux_max.value)\n",
    "#             if len(x_max)>1 or len(y_max)>1:\n",
    "#                 x_max, y_max = np.nanmean(x_max), np.nanmean(y_max)\n",
    "#             else:\n",
    "#                 x_max, y_max = x_max[0], y_max[0]\n",
    "\n",
    "#             data_zeros = data_masked.copy()\n",
    "#             data_zeros[np.isnan(data_zeros)] = 0\n",
    "#             x_com, y_com = ndimage.center_of_mass(data_zeros)\n",
    "\n",
    "#             wcs = WCS(header)\n",
    "#             ra_max, dec_max = wcs.array_index_to_world_values([[y_max, x_max]])[0] * au.deg\n",
    "#             ra_com, dec_com = wcs.array_index_to_world_values([[y_com, x_com]])[0] * au.deg\n",
    "\n",
    "#             table_data = [\n",
    "#                 regionID, x_max, y_max, x_com, y_com, \n",
    "#                 ra_max, dec_max, ra_com, dec_com,\n",
    "#                 npix, flux, flux_err, area_exact, radius_circ, flux_max, flux_min, flux_mean\n",
    "#             ]\n",
    "#             table_data = [np.array(td) for td in table_data]\n",
    "#             table_names = [\n",
    "#                 'region_ID', 'x_max', 'y_max', 'x_com', 'y_com', \n",
    "#                 'ra_max', 'dec_max', 'ra_com', 'dec_com',\n",
    "#                 'npix', 'flux', 'flux_err', 'area_exact', 'radius_circ', \n",
    "#                 'flux_max', 'flux_min', 'flux_mean'\n",
    "#             ]\n",
    "\n",
    "#             props = QTable(np.array(table_data), names=table_names)\n",
    "#             props['x_max'].unit       = au.pix\n",
    "#             props['y_max'].unit       = au.pix\n",
    "#             props['x_com'].unit       = au.pix\n",
    "#             props['y_com'].unit       = au.pix\n",
    "#             props['ra_max'].unit      = au.deg\n",
    "#             props['dec_max'].unit     = au.deg\n",
    "#             props['ra_com'].unit      = au.deg\n",
    "#             props['dec_com'].unit     = au.deg\n",
    "#             props['npix'].unit        = au.pix\n",
    "#             props['flux'].unit        = flux.unit\n",
    "#             props['flux_err'].unit    = flux.unit\n",
    "#             props['area_exact'].unit  = au.arcsec**2\n",
    "#             props['radius_circ'].unit = au.arcsec\n",
    "#             props['flux_max'].unit    = flux_max.unit\n",
    "#             props['flux_min'].unit    = flux_min.unit\n",
    "#             props['flux_mean'].unit   = flux_mean.unit\n",
    "\n",
    "#             # Convert radius_circ to parsecs\n",
    "#             pcperarcsec = cat_props.get_pcperarcsec(self.sample_table)\n",
    "#             props['radius_circ_pc'] = props['radius_circ']*pcperarcsec\n",
    "\n",
    "#             # Dendrogram analysis (very lenient params)\n",
    "#             # data_masked_zero = data_masked.copy()\n",
    "#             # data_masked_zero[np.isnan(data_masked_zero)] = 0\n",
    "\n",
    "#             dendro = Dendrogram.compute(\n",
    "#                 data_masked, \n",
    "#                 min_delta=-1000, \n",
    "#                 min_value=0, \n",
    "#                 min_npix=0, \n",
    "#                 wcs=wcs\n",
    "#             )\n",
    "#             metadata = {\n",
    "#                 'data_unit': au.Jy/au.beam,\n",
    "#                 'spatial_scale': pixsize.to('arcsec'),\n",
    "#                 'beam_major': 0.1*au.arcsec,\n",
    "#                 'beam_minor': 0.1*au.arcsec\n",
    "#             }\n",
    "\n",
    "#             props_dendro = pp_catalog(dendro.trunk, metadata, verbose=False)  \n",
    "#             props_dendro = QTable(props_dendro)\n",
    "#             props_dendro = props_dendro[np.nanargmax(props_dendro['flux'])]\n",
    "\n",
    "#             ra_dendro, dec_dendro = wcs.array_index_to_world_values(\n",
    "#                 [[props_dendro['x_cen'].value, props_dendro['y_cen'].value]]\n",
    "#             )[0] * au.deg\n",
    "\n",
    "#             props['x_mom']          = props_dendro['x_cen']\n",
    "#             props['y_mom']          = props_dendro['y_cen']\n",
    "#             props['ra_mom']         = ra_dendro\n",
    "#             props['dec_mom']        = dec_dendro\n",
    "#             props['area_ellipse']   = props_dendro['area_ellipse']\n",
    "#             props['major_sigma']    = props_dendro['major_sigma']\n",
    "#             props['minor_sigma']    = props_dendro['minor_sigma']\n",
    "#             props['mean_sigma']     = props_dendro['radius']\n",
    "#             props['position_angle'] = props_dendro['position_angle']\n",
    "#             props['mean_sigma_pc']  = props['mean_sigma'] * pcperarcsec\n",
    "\n",
    "#             # Dendrogram complexity with higher thresholds\n",
    "#             dendro = Dendrogram.compute(\n",
    "#                 data_masked,\n",
    "#                 min_delta=(self.hstha_err*3),\n",
    "#                 min_value=(self.hstha_err),\n",
    "#                 min_npix=9,\n",
    "#                 wcs=wcs\n",
    "#             )\n",
    "#             dendro_IDs = np.unique(dendro.index_map.data)\n",
    "#             dendro_IDs = [dID for dID in dendro_IDs if dID > -1]\n",
    "\n",
    "#             dendro_complex = len(dendro_IDs)\n",
    "#             dendro_complex_leaves = len(dendro.leaves)\n",
    "#             complexity_rms = np.sqrt(np.nanmean((data_masked.copy())**2))\n",
    "#             complexity_std = np.nanstd(data_masked.copy())\n",
    "\n",
    "#             props['complexity_score'] = dendro_complex\n",
    "#             props['complexity_score_leaves'] = dendro_complex_leaves\n",
    "#             props['complexity_rms'] = complexity_rms\n",
    "#             props['complexity_std'] = complexity_std\n",
    "\n",
    "#             # Edge & Touch flags\n",
    "#             flag_edge = np.isnan(hdus['hstha_hdu_masked_ones'][i].data).any()*1\n",
    "#             props.add_column(Column(flag_edge, name='flag_edge_hst'))\n",
    "\n",
    "#             mask_touch = (ndimage.binary_dilation(mask_final) & ~mask_final)\n",
    "#             flag_touch = (np.nansum(np.isnan(data[mask_touch])) > 0)*1\n",
    "#             props.add_column(Column(flag_touch, name='flag_touch_hst'))\n",
    "\n",
    "#             props_all.append(props)\n",
    "\n",
    "#         # Stack all region properties\n",
    "#         props_all = vstack(props_all)\n",
    "\n",
    "#         # Merge with MUSE data (region_ID-based)\n",
    "#         self.muscat_table = cat_misc.get_museprops(self.galaxy, self.muscat_table_file)\n",
    "#         self.muscat_table_rename = self.muscat_table.copy()\n",
    "#         for column in self.muscat_table_rename.colnames:\n",
    "#             self.muscat_table_rename.rename_column(column, column+'_MUSE')\n",
    "#         self.muscat_table_rename.rename_column('gal_name_MUSE', 'gal_name')\n",
    "#         self.muscat_table_rename.rename_column('region_ID_MUSE', 'region_ID')\n",
    "#         self.muscat_table_rename.rename_column('Lum_HA6562_CORR_MUSE', 'HA6562_LUMINOSITY_MUSE')\n",
    "\n",
    "#         props_all_final = join(props_all, self.muscat_table_rename, keys='region_ID')\n",
    "\n",
    "#         # Correct fluxes & luminosities\n",
    "#         props_all_final['flux_corr'] = cat_props.correct_ha_flux(\n",
    "#             props_all_final, props_all_final['flux']\n",
    "#         )\n",
    "#         props_all_final['flux_err_corr'] = cat_props.correct_ha_flux(\n",
    "#             props_all_final, props_all_final['flux_err']\n",
    "#         )\n",
    "#         dist_val = self.sample_table['dist'][0]  \n",
    "#         props_all_final['lum_hst'] = cat_props.calculate_luminosity(\n",
    "#             props_all_final['flux_corr']*1e-20, dist_val\n",
    "#         )\n",
    "#         props_all_final['lum_err_hst'] = cat_props.calculate_luminosity(\n",
    "#             props_all_final['flux_err_corr']*1e-20, dist_val\n",
    "#         )\n",
    "#         props_all_final['region_circ_rad_pc_MUSE'] = cat_props.calculate_radius(\n",
    "#             props_all_final['region_circ_rad_MUSE'], dist_val\n",
    "#         )\n",
    "\n",
    "#         # Rename columns\n",
    "#         props_all_final.rename_column('flux',          'HA6562_FLUX_HST')\n",
    "#         props_all_final.rename_column('flux_err',      'HA6562_FLUX_ERR_HST')\n",
    "#         props_all_final.rename_column('flux_corr',     'HA6562_FLUX_CORR_HST')\n",
    "#         props_all_final.rename_column('flux_err_corr', 'HA6562_FLUX_ERR_CORR_HST')\n",
    "#         props_all_final.rename_column('lum_hst',       'HA6562_LUMINOSITY_HST')\n",
    "#         props_all_final.rename_column('lum_err_hst',   'HA6562_LUMINOSITY_ERR_HST')\n",
    "\n",
    "#         self.props = props_all_final\n",
    "\n",
    "#         # Save final table\n",
    "#         props_file = f'{self.catalogue_dir}/props_all.fits'\n",
    "#         self.props.write(props_file, overwrite=True)\n",
    "#         print(f'[CATALOGUE] Final properties table saved to: {props_file}')\n",
    "\n",
    "#         # Save masks\n",
    "#         mask_file = f'{self.catalogue_dir}/{self.galaxy}_mask.fits'\n",
    "#         cat_mask.get_hdumask(self.hstha_hdu, hdus_mask_id, outputfile=mask_file)\n",
    "\n",
    "#         complexity_file = f'{self.catalogue_dir}/{self.galaxy}_complexity.fits'\n",
    "#         cat_mask.get_hducomplex(self.props, inputfile=mask_file,\n",
    "#                                 outputfile=complexity_file)\n",
    "\n",
    "#         regions_out = f'{self.catalogue_dir}/{self.galaxy}_mask'\n",
    "#         cat_mask.get_ds9regions(props_all, outputfile=regions_out)\n",
    "\n",
    "#         print(f'[CATALOGUE] Masks saved to {mask_file} and {complexity_file}.')\n",
    "#         print(f'[CATALOGUE] DS9 regions output to {regions_out}.')\n",
    "#         print('[CATALOGUE] Done!')\n",
    "\n",
    "#     def make_associations(self):\n",
    "#         \"\"\"\n",
    "#         Build an extended table that associates each nebula region_ID with \n",
    "#         stellar associations at multiple physical scales (v_08pc, nuv_16pc, etc.).\n",
    "        \n",
    "#         1. Check if association files exist, unpack them if needed.\n",
    "#         2. Read in the association tables & masks.\n",
    "#         3. Load the main nebula props table ('props_all.fits') and region definitions.\n",
    "#         4. Create or load cutouts for each association mask if 'self.rerun_cutouts' is True.\n",
    "#         5. For each region, compute which association IDs fall inside.\n",
    "#         6. Join them into one final table, 'props_all_association.fits'.\n",
    "#         7. Clean up (e.g., remove unpacked temp files).\n",
    "#         \"\"\"\n",
    "\n",
    "#         # -----------------------------------------------------------------\n",
    "#         # 1. Define file lists for associations and check existence/unpack\n",
    "#         # -----------------------------------------------------------------\n",
    "#         assoc_files = [\n",
    "#             self.association_table_file_v_08pc,   self.association_table_file_v_16pc,\n",
    "#             self.association_table_file_v_32pc,   self.association_table_file_v_64pc,\n",
    "#             self.association_table_file_nuv_08pc, self.association_table_file_nuv_16pc,\n",
    "#             self.association_table_file_nuv_32pc, self.association_table_file_nuv_64pc,\n",
    "#             self.association_mask_file_v_08pc,    self.association_mask_file_v_16pc,\n",
    "#             self.association_mask_file_v_32pc,    self.association_mask_file_v_64pc,\n",
    "#             self.association_mask_file_nuv_08pc,  self.association_mask_file_nuv_16pc,\n",
    "#             self.association_mask_file_nuv_32pc,  self.association_mask_file_nuv_64pc\n",
    "#         ]\n",
    "\n",
    "#         print(f\"[ASSOCIATIONS] Checking association files for galaxy {self.galaxy} ...\")\n",
    "#         for file in assoc_files:\n",
    "#             cat_misc.get_unpack(file)  # Unpack if needed\n",
    "#             if os.path.isfile(file):\n",
    "#                 print(f\"   [OK] File found: {file}\")\n",
    "#             else:\n",
    "#                 print(f\"   [!!] File not found: {file}\")\n",
    "\n",
    "#         # -----------------------------------------------------------------\n",
    "#         # 2. Read the association FITS tables & masks\n",
    "#         # -----------------------------------------------------------------\n",
    "#         props_associations_v_08pc   = QTable.read(self.association_table_file_v_08pc)\n",
    "#         props_associations_v_16pc   = QTable.read(self.association_table_file_v_16pc)\n",
    "#         props_associations_v_32pc   = QTable.read(self.association_table_file_v_32pc)\n",
    "#         props_associations_v_64pc   = QTable.read(self.association_table_file_v_64pc)\n",
    "\n",
    "#         props_associations_nuv_08pc = QTable.read(self.association_table_file_nuv_08pc)\n",
    "#         props_associations_nuv_16pc = QTable.read(self.association_table_file_nuv_16pc)\n",
    "#         props_associations_nuv_32pc = QTable.read(self.association_table_file_nuv_32pc)\n",
    "#         props_associations_nuv_64pc = QTable.read(self.association_table_file_nuv_64pc)\n",
    "\n",
    "#         hdu_association_mask_v_08pc   = fits.open(self.association_mask_file_v_08pc)[0]\n",
    "#         hdu_association_mask_v_16pc   = fits.open(self.association_mask_file_v_16pc)[0]\n",
    "#         hdu_association_mask_v_32pc   = fits.open(self.association_mask_file_v_32pc)[0]\n",
    "#         hdu_association_mask_v_64pc   = fits.open(self.association_mask_file_v_64pc)[0]\n",
    "#         hdu_association_mask_nuv_08pc = fits.open(self.association_mask_file_nuv_08pc)[0]\n",
    "#         hdu_association_mask_nuv_16pc = fits.open(self.association_mask_file_nuv_16pc)[0]\n",
    "#         hdu_association_mask_nuv_32pc = fits.open(self.association_mask_file_nuv_32pc)[0]\n",
    "#         hdu_association_mask_nuv_64pc = fits.open(self.association_mask_file_nuv_64pc)[0]\n",
    "\n",
    "#         # -----------------------------------------------------------------\n",
    "#         # 3. Load your final nebula table ('props_all.fits') and region definitions\n",
    "#         #    or we can just use self.props_all_final if you've already got it in memory.\n",
    "#         #    Here we mirror your snippet, reading from disk.\n",
    "#         # -----------------------------------------------------------------\n",
    "#         props_all_path = f\"{self.catalogue_dir}/props_all.fits\"\n",
    "#         props_all = QTable.read(props_all_path)  # The main nebula catalog\n",
    "\n",
    "#         regions = cat_misc.load_pickle(self.regions_pickle_file)\n",
    "#         hdu_catalog_mask = fits.open(self.catalog_mask_file)[0]\n",
    "\n",
    "#         # Potentially filter regions down to those in props_all\n",
    "#         region_IDs = props_all['region_ID'].data\n",
    "#         for key in regions.keys():\n",
    "#             # Keep only region indices that appear in props_all\n",
    "#             subset = [int(rid) for rid in region_IDs]\n",
    "#             regions[key] = [regions[key][s] for s in subset]\n",
    "\n",
    "#         # -----------------------------------------------------------------\n",
    "#         # 4. Create or load the association-mask cutouts for each region\n",
    "#         # -----------------------------------------------------------------\n",
    "#         if self.rerun_cutouts_associations:\n",
    "\n",
    "#             print(\"[ASSOCIATIONS] Generating new cutouts for association masks...\")\n",
    "#             hdus_association_mask_new_v_08pc   = cat_cutouts.get_croppeddata_all(hdu_association_mask_v_08pc,   regions)\n",
    "#             hdus_association_mask_new_v_16pc   = cat_cutouts.get_croppeddata_all(hdu_association_mask_v_16pc,   regions)\n",
    "#             hdus_association_mask_new_v_32pc   = cat_cutouts.get_croppeddata_all(hdu_association_mask_v_32pc,   regions)\n",
    "#             hdus_association_mask_new_v_64pc   = cat_cutouts.get_croppeddata_all(hdu_association_mask_v_64pc,   regions)\n",
    "\n",
    "#             hdus_association_mask_new_nuv_08pc = cat_cutouts.get_croppeddata_all(hdu_association_mask_nuv_08pc, regions)\n",
    "#             hdus_association_mask_new_nuv_16pc = cat_cutouts.get_croppeddata_all(hdu_association_mask_nuv_16pc, regions)\n",
    "#             hdus_association_mask_new_nuv_32pc = cat_cutouts.get_croppeddata_all(hdu_association_mask_nuv_32pc, regions)\n",
    "#             hdus_association_mask_new_nuv_64pc = cat_cutouts.get_croppeddata_all(hdu_association_mask_nuv_64pc, regions)\n",
    "\n",
    "#             hdus_catalog_mask_new = cat_cutouts.get_croppeddata_all(hdu_catalog_mask, regions)\n",
    "\n",
    "#             # Save pickles\n",
    "#             cat_misc.save_pickle(hdus_association_mask_new_v_08pc,   f\"{self.cutout_dir}/hdus_association_mask_new_v_08pc.pickel\")\n",
    "#             cat_misc.save_pickle(hdus_association_mask_new_v_16pc,   f\"{self.cutout_dir}/hdus_association_mask_new_v_16pc.pickel\")\n",
    "#             cat_misc.save_pickle(hdus_association_mask_new_v_32pc,   f\"{self.cutout_dir}/hdus_association_mask_new_v_32pc.pickel\")\n",
    "#             cat_misc.save_pickle(hdus_association_mask_new_v_64pc,   f\"{self.cutout_dir}/hdus_association_mask_new_v_64pc.pickel\")\n",
    "\n",
    "#             cat_misc.save_pickle(hdus_association_mask_new_nuv_08pc, f\"{self.cutout_dir}/hdus_association_mask_new_nuv_08pc.pickel\")\n",
    "#             cat_misc.save_pickle(hdus_association_mask_new_nuv_16pc, f\"{self.cutout_dir}/hdus_association_mask_new_nuv_16pc.pickel\")\n",
    "#             cat_misc.save_pickle(hdus_association_mask_new_nuv_32pc, f\"{self.cutout_dir}/hdus_association_mask_new_nuv_32pc.pickel\")\n",
    "#             cat_misc.save_pickle(hdus_association_mask_new_nuv_64pc, f\"{self.cutout_dir}/hdus_association_mask_new_nuv_64pc.pickel\")\n",
    "\n",
    "#             cat_misc.save_pickle(hdus_catalog_mask_new,              f\"{self.cutout_dir}/hdus_catalog_mask_new.pickel\")\n",
    "\n",
    "#         else:\n",
    "#             print(\"[ASSOCIATIONS] Loading existing association mask cutouts...\")\n",
    "#             hdus_association_mask_new_v_08pc   = cat_misc.load_pickle(f\"{self.cutout_dir}/hdus_association_mask_new_v_08pc.pickel\")\n",
    "#             hdus_association_mask_new_v_16pc   = cat_misc.load_pickle(f\"{self.cutout_dir}/hdus_association_mask_new_v_16pc.pickel\")\n",
    "#             hdus_association_mask_new_v_32pc   = cat_misc.load_pickle(f\"{self.cutout_dir}/hdus_association_mask_new_v_32pc.pickel\")\n",
    "#             hdus_association_mask_new_v_64pc   = cat_misc.load_pickle(f\"{self.cutout_dir}/hdus_association_mask_new_v_64pc.pickel\")\n",
    "\n",
    "#             hdus_association_mask_new_nuv_08pc = cat_misc.load_pickle(f\"{self.cutout_dir}/hdus_association_mask_new_nuv_08pc.pickel\")\n",
    "#             hdus_association_mask_new_nuv_16pc = cat_misc.load_pickle(f\"{self.cutout_dir}/hdus_association_mask_new_nuv_16pc.pickel\")\n",
    "#             hdus_association_mask_new_nuv_32pc = cat_misc.load_pickle(f\"{self.cutout_dir}/hdus_association_mask_new_nuv_32pc.pickel\")\n",
    "#             hdus_association_mask_new_nuv_64pc = cat_misc.load_pickle(f\"{self.cutout_dir}/hdus_association_mask_new_nuv_64pc.pickel\")\n",
    "\n",
    "#             hdus_catalog_mask_new = cat_misc.load_pickle(f\"{self.cutout_dir}/hdus_catalog_mask_new.pickel\")\n",
    "\n",
    "#         # -----------------------------------------------------------------\n",
    "#         # 5. For each region in the nebula catalog, find which association \n",
    "#         #    IDs overlap, then build a big table of results.\n",
    "#         # -----------------------------------------------------------------\n",
    "#         props_associations_all = []\n",
    "\n",
    "#         for i in tqdm(range(len(props_all['region_ID'])), desc='Match associations:', position=0):\n",
    "#             region_ID = props_all['region_ID'][i]\n",
    "\n",
    "#             # Get association IDs by masking the association mask with the region ID mask...\n",
    "#             association_IDs_v_08pc = cat_associations.get_association_IDs(\n",
    "#                 region_ID, i, hdus_catalog_mask_new, hdus_association_mask_new_v_08pc\n",
    "#             )\n",
    "#             association_IDs_v_16pc = cat_associations.get_association_IDs(\n",
    "#                 region_ID, i, hdus_catalog_mask_new, hdus_association_mask_new_v_16pc\n",
    "#             )\n",
    "#             association_IDs_v_32pc = cat_associations.get_association_IDs(\n",
    "#                 region_ID, i, hdus_catalog_mask_new, hdus_association_mask_new_v_32pc\n",
    "#             )\n",
    "#             association_IDs_v_64pc = cat_associations.get_association_IDs(\n",
    "#                 region_ID, i, hdus_catalog_mask_new, hdus_association_mask_new_v_64pc\n",
    "#             )\n",
    "\n",
    "#             association_IDs_nuv_08pc = cat_associations.get_association_IDs(\n",
    "#                 region_ID, i, hdus_catalog_mask_new, hdus_association_mask_new_nuv_08pc\n",
    "#             )\n",
    "#             association_IDs_nuv_16pc = cat_associations.get_association_IDs(\n",
    "#                 region_ID, i, hdus_catalog_mask_new, hdus_association_mask_new_nuv_16pc\n",
    "#             )\n",
    "#             association_IDs_nuv_32pc = cat_associations.get_association_IDs(\n",
    "#                 region_ID, i, hdus_catalog_mask_new, hdus_association_mask_new_nuv_32pc\n",
    "#             )\n",
    "#             association_IDs_nuv_64pc = cat_associations.get_association_IDs(\n",
    "#                 region_ID, i, hdus_catalog_mask_new, hdus_association_mask_new_nuv_64pc\n",
    "#             )\n",
    "\n",
    "#             # Now gather up the matching association properties in each scale/selection\n",
    "#             props_associations_new_v_08pc = cat_associations.get_all_associations(\n",
    "#                 region_ID, association_IDs_v_08pc, props_associations_v_08pc, 'v_08pc'\n",
    "#             )\n",
    "#             props_associations_new_v_16pc = cat_associations.get_all_associations(\n",
    "#                 region_ID, association_IDs_v_16pc, props_associations_v_16pc, 'v_16pc'\n",
    "#             )\n",
    "#             props_associations_new_v_32pc = cat_associations.get_all_associations(\n",
    "#                 region_ID, association_IDs_v_32pc, props_associations_v_32pc, 'v_32pc'\n",
    "#             )\n",
    "#             props_associations_new_v_64pc = cat_associations.get_all_associations(\n",
    "#                 region_ID, association_IDs_v_64pc, props_associations_v_64pc, 'v_64pc'\n",
    "#             )\n",
    "\n",
    "#             props_associations_new_nuv_08pc = cat_associations.get_all_associations(\n",
    "#                 region_ID, association_IDs_nuv_08pc, props_associations_nuv_08pc, 'nuv_08pc'\n",
    "#             )\n",
    "#             props_associations_new_nuv_16pc = cat_associations.get_all_associations(\n",
    "#                 region_ID, association_IDs_nuv_16pc, props_associations_nuv_16pc, 'nuv_16pc'\n",
    "#             )\n",
    "#             props_associations_new_nuv_32pc = cat_associations.get_all_associations(\n",
    "#                 region_ID, association_IDs_nuv_32pc, props_associations_nuv_32pc, 'nuv_32pc'\n",
    "#             )\n",
    "#             props_associations_new_nuv_64pc = cat_associations.get_all_associations(\n",
    "#                 region_ID, association_IDs_nuv_64pc, props_associations_nuv_64pc, 'nuv_64pc'\n",
    "#             )\n",
    "\n",
    "#             # Combine them all into one row for this region\n",
    "#             props_associations_all_ = join(props_associations_new_v_08pc, props_associations_new_v_16pc, keys='region_ID')\n",
    "#             props_associations_all_ = join(props_associations_all_, props_associations_new_v_32pc, keys='region_ID')\n",
    "#             props_associations_all_ = join(props_associations_all_, props_associations_new_v_64pc, keys='region_ID')\n",
    "#             props_associations_all_ = join(props_associations_all_, props_associations_new_nuv_08pc, keys='region_ID')\n",
    "#             props_associations_all_ = join(props_associations_all_, props_associations_new_nuv_16pc, keys='region_ID')\n",
    "#             props_associations_all_ = join(props_associations_all_, props_associations_new_nuv_32pc, keys='region_ID')\n",
    "#             props_associations_all_ = join(props_associations_all_, props_associations_new_nuv_64pc, keys='region_ID')\n",
    "\n",
    "#             props_associations_all.append(props_associations_all_)\n",
    "\n",
    "#         # -----------------------------------------------------------------\n",
    "#         # 6. Stack them into one big table and join with the nebula table\n",
    "#         # -----------------------------------------------------------------\n",
    "#         props_associations_stacked = vstack(props_associations_all)\n",
    "#         self.props_associations = join(self.props, props_associations_stacked, keys='region_ID')\n",
    "\n",
    "#         # -----------------------------------------------------------------\n",
    "#         # 7. Save final association-augmented catalog and clean up\n",
    "#         # -----------------------------------------------------------------\n",
    "#         out_fits = f\"{self.catalogue_dir}/props_all_association.fits\"\n",
    "#         self.props_associations.write(out_fits, overwrite=True)\n",
    "#         print(f\"[ASSOCIATIONS] Final association table saved to: {out_fits}\")\n",
    "\n",
    "#         # Clean up (pack files again to save space, if that's your workflow)\n",
    "#         for file in assoc_files:\n",
    "#             cat_misc.clean_unpack(file)\n",
    "\n",
    "#         print(\"[ASSOCIATIONS] Done building the multi-scale association catalog!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Outputting to the following:\n",
      "/Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/\n",
      "[INFO] [get_MuseProps] Getting MUSE catalouge properties for ic5332...\n",
      "[INFO] [get_galaxyprops] Getting sample table properties for ic5332...\n",
      "[INFO] Using existing region file: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/sample.reg\n",
      "[INFO] [load_pickle] Load /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/sample.pickel\n",
      "[INFO] All cutouts already exist. Loading from file /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/hdus_all.pickel\n",
      "[INFO] [load_pickle] Load /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/hdus_all.pickel\n",
      "[CATALOGUE] Running catalogue generation for: ic5332\n",
      "[INFO] [load_pickle] Load /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/hdus_all_withmasked.pickel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get sources:: 100%|██████████| 816/816 [00:07<00:00, 106.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] [get_MuseProps] Getting MUSE catalouge properties for ic5332...\n",
      "[CATALOGUE] Final properties table saved to: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/catalogue/props_all.fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Masking regions: 100%|██████████| 816/816 [00:00<00:00, 1488.22it/s]\n",
      "100%|██████████| 162/162 [00:03<00:00, 44.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CATALOGUE] Masks saved to /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/catalogue/ic5332_mask.fits and /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/catalogue/ic5332_complexity.fits.\n",
      "[CATALOGUE] DS9 regions output to /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/catalogue/ic5332_mask.\n",
      "[CATALOGUE] Done!\n",
      "[ASSOCIATIONS] Checking association files for galaxy ic5332 ...\n",
      "File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws8pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws8pc-main.fits\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws8pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws8pc-main.fits\n",
      "File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws16pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws16pc-main.fits\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws16pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws16pc-main.fits\n",
      "File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws32pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws32pc-main.fits\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws32pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws32pc-main.fits\n",
      "File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws64pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws64pc-main.fits\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws64pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws64pc-main.fits\n",
      "File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws8pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws8pc-main.fits\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws8pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws8pc-main.fits\n",
      "File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws16pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws16pc-main.fits\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws16pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws16pc-main.fits\n",
      "File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws32pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws32pc-main.fits\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws32pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws32pc-main.fits\n",
      "File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws64pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws64pc-main.fits\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws64pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws64pc-main.fits\n",
      "Unpacking: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws8pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws8pc-idmask.fits.gz\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws8pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws8pc-idmask.fits\n",
      "Unpacking: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws16pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws16pc-idmask.fits.gz\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws16pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws16pc-idmask.fits\n",
      "Unpacking: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws32pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws32pc-idmask.fits.gz\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws32pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws32pc-idmask.fits\n",
      "Unpacking: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws64pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws64pc-idmask.fits.gz\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws64pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws64pc-idmask.fits\n",
      "Unpacking: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws8pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws8pc-idmask.fits.gz\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws8pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws8pc-idmask.fits\n",
      "Unpacking: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws16pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws16pc-idmask.fits.gz\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws16pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws16pc-idmask.fits\n",
      "Unpacking: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws32pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws32pc-idmask.fits.gz\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws32pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws32pc-idmask.fits\n",
      "Unpacking: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws64pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws64pc-idmask.fits.gz\n",
      "   [OK] File found: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws64pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws64pc-idmask.fits\n",
      "[INFO] [load_pickle] Load /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/sample.pickel\n",
      "[ASSOCIATIONS] Loading existing association mask cutouts...\n",
      "[INFO] [load_pickle] Load /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/hdus_association_mask_new_v_08pc.pickel\n",
      "[INFO] [load_pickle] Load /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/hdus_association_mask_new_v_16pc.pickel\n",
      "[INFO] [load_pickle] Load /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/hdus_association_mask_new_v_32pc.pickel\n",
      "[INFO] [load_pickle] Load /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/hdus_association_mask_new_v_64pc.pickel\n",
      "[INFO] [load_pickle] Load /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/hdus_association_mask_new_nuv_08pc.pickel\n",
      "[INFO] [load_pickle] Load /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/hdus_association_mask_new_nuv_16pc.pickel\n",
      "[INFO] [load_pickle] Load /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/hdus_association_mask_new_nuv_32pc.pickel\n",
      "[INFO] [load_pickle] Load /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/hdus_association_mask_new_nuv_64pc.pickel\n",
      "[INFO] [load_pickle] Load /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/cutouts/hdus_catalog_mask_new.pickel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Match associations:: 100%|██████████| 162/162 [00:26<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ASSOCIATIONS] Final association table saved to: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_hstha_nebulae_catalogue/ic5332/catalogue/props_all_association.fits\n",
      "Cleaning: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws8pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws8pc-idmask.fits\n",
      "Cleaning: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws16pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws16pc-idmask.fits\n",
      "Cleaning: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws32pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws32pc-idmask.fits\n",
      "Cleaning: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/vselect/ws64pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-vselect-ws64pc-idmask.fits\n",
      "Cleaning: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws8pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws8pc-idmask.fits\n",
      "Cleaning: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws16pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws16pc-idmask.fits\n",
      "Cleaning: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws32pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws32pc-idmask.fits\n",
      "Cleaning: /Users/abarnes/Dropbox/work/Smallprojects/galaxies/data_misc/multiscale_stellar_associations/ic5332/nuvselect/ws64pc/PHANGS_IR4_hst_wfc3_ic5332_v1p3_multi_assoc-nuvselect-ws64pc-idmask.fits\n",
      "[ASSOCIATIONS] Done building the multi-scale association catalog!\n"
     ]
    }
   ],
   "source": [
    "my_cat = PyHSTHACat(galaxy='ic5332')\n",
    "my_cat.make_paths()\n",
    "my_cat.load_files()\n",
    "my_cat.get_regions()\n",
    "my_cat.get_cutouts()\n",
    "my_cat.make_catalogue()\n",
    "# my_cat.rerun_cutouts_associations = True\n",
    "my_cat.make_associations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
